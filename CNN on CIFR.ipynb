{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK3alCdFflQX"
   },
   "source": [
    "<h1 >CNN on CIFR </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHCYMwwXflQd"
   },
   "source": [
    "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
    "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
    "3.  You cannot use Dense Layers (also called fully connected layers), or DropOut.\n",
    "4.  You MUST use Image Augmentation Techniques.\n",
    "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
    "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
    "7.  You cannot use test images for training the model.\n",
    "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
    "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
    "10. You cannot have more than 1 Million parameters in total\n",
    "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
    "12. You can use any optimization algorithm you need. \n",
    "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TLVcyNYKflQi",
    "outputId": "cccf4515-27ab-40ac-b4e9-57904ce4baaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h0ltb8tXrLug"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "l = 5\n",
    "num_filter =40\n",
    "\n",
    "epochs = 100\n",
    "num_classes = 10\n",
    "compression = 1\n",
    "dropout_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VMErk02xrLui",
    "outputId": "63df5d0c-6f7a-4622-9804-7baa823f8f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JzLQvwhrrLul"
   },
   "source": [
    "<h1> Image preprocessing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fgkrx_pSrLul"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=15,width_shift_range=0.1,height_shift_range=0.1,horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "train_data=datagen.flow(X_train,y_train, batch_size=batch_size)\n",
    "val_data=datagen.flow(X_test,y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zXyqSfzrLur"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def denseblock(input, num_filter, dropout_rate):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)                         \n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        temp = concat\n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter,dropout_rate ):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(1,1))(relu)\n",
    "    output = layers.Conv2D(num_classes,kernel_size=[4,4],strides=(1,1), activation='softmax')(AvgPooling)\n",
    "    flat_output = layers.Flatten()(output)\n",
    "    return flat_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FWM_M0lrLuv"
   },
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "afLOV2dSrLuz",
    "outputId": "b8ffa488-be0a-4e66-9cc2-2c9acba59cc7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 40)   1080        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 40)   160         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 40)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 40)   14400       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 80)   0           conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 80)   320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 80)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 40)   28800       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 120)  0           concatenate[0][0]                \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 120)  480         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 120)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 40)   43200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 160)  0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 40)   57600       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 200)  0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 200)  800         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 200)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 40)   72000       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 240)  0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 240)  960         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 240)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 40)   9600        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 40)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 40)   160         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 40)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 40)   14400       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 80)   0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 80)   320         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 80)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 40)   28800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 120)  0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 120)  480         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 120)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 40)   43200       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 160)  0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 160)  640         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 160)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 40)   57600       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 200)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 200)  800         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 200)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 40)   72000       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 240)  0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 240)  960         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 240)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 40)   9600        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 40)     0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 40)     160         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 40)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 40)     14400       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8, 8, 80)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 80)     320         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 80)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 40)     28800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 8, 8, 120)    0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 120)    480         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 120)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 40)     43200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 8, 8, 160)    0           concatenate_11[0][0]             \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 160)    640         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 160)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 40)     57600       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 8, 8, 200)    0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 200)    800         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 200)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 40)     72000       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 8, 240)    0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 240)    960         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 240)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 40)     9600        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 40)     0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 4, 4, 40)     160         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 4, 4, 40)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 4, 40)     14400       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 4, 4, 80)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 4, 4, 80)     320         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 4, 4, 80)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 4, 4, 40)     28800       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 4, 4, 120)    0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 4, 4, 120)    480         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 4, 4, 120)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 4, 4, 40)     43200       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 4, 4, 160)    0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 4, 4, 160)    640         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 4, 4, 160)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 4, 4, 40)     57600       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 4, 4, 200)    0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 4, 200)    800         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 200)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 4, 40)     72000       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 4, 4, 240)    0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 240)    960         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 240)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 4, 4, 240)    0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 1, 1, 10)     38410       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10)           0           conv2d_24[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 945,730\n",
      "Trainable params: 939,010\n",
      "Non-trainable params: 6,720\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UB9_iA4erLu5"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uQyFvrs-rLu9",
    "outputId": "9353375b-2922-4746-9fff-1c6a62367b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "782/782 [==============================] - 39s 50ms/step - loss: 1.4061 - accuracy: 0.5072 - val_loss: 1.9530 - val_accuracy: 0.5024\n",
      "Epoch 2/120\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.9944 - accuracy: 0.6510 - val_loss: 1.2105 - val_accuracy: 0.6008\n",
      "Epoch 3/120\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.8126 - accuracy: 0.7151 - val_loss: 1.2512 - val_accuracy: 0.5909\n",
      "Epoch 4/120\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.7098 - accuracy: 0.7525 - val_loss: 0.8436 - val_accuracy: 0.7061\n",
      "Epoch 5/120\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.6376 - accuracy: 0.7766 - val_loss: 0.8907 - val_accuracy: 0.7092\n",
      "Epoch 6/120\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.5822 - accuracy: 0.7983 - val_loss: 0.7485 - val_accuracy: 0.7471\n",
      "Epoch 7/120\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.5377 - accuracy: 0.8141 - val_loss: 0.6789 - val_accuracy: 0.7695\n",
      "Epoch 8/120\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.4980 - accuracy: 0.8289 - val_loss: 0.6220 - val_accuracy: 0.7893\n",
      "Epoch 9/120\n",
      "782/782 [==============================] - 37s 48ms/step - loss: 0.4615 - accuracy: 0.8411 - val_loss: 0.6508 - val_accuracy: 0.7783\n",
      "Epoch 10/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.4383 - accuracy: 0.8491 - val_loss: 0.6050 - val_accuracy: 0.7927\n",
      "Epoch 11/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.4141 - accuracy: 0.8575 - val_loss: 0.5111 - val_accuracy: 0.8258\n",
      "Epoch 12/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.3880 - accuracy: 0.8654 - val_loss: 0.5114 - val_accuracy: 0.8270\n",
      "Epoch 13/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.3714 - accuracy: 0.8708 - val_loss: 0.6425 - val_accuracy: 0.7832\n",
      "Epoch 14/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.3545 - accuracy: 0.8758 - val_loss: 0.5183 - val_accuracy: 0.8321\n",
      "Epoch 15/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.3385 - accuracy: 0.8812 - val_loss: 0.4934 - val_accuracy: 0.8359\n",
      "Epoch 16/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.3213 - accuracy: 0.8894 - val_loss: 0.6566 - val_accuracy: 0.7891\n",
      "Epoch 17/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.3120 - accuracy: 0.8915 - val_loss: 0.4838 - val_accuracy: 0.8356\n",
      "Epoch 18/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2996 - accuracy: 0.8958 - val_loss: 0.4555 - val_accuracy: 0.8457\n",
      "Epoch 19/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2880 - accuracy: 0.9003 - val_loss: 0.4933 - val_accuracy: 0.8343\n",
      "Epoch 20/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2717 - accuracy: 0.9056 - val_loss: 0.4409 - val_accuracy: 0.8523\n",
      "Epoch 21/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2639 - accuracy: 0.9091 - val_loss: 0.4279 - val_accuracy: 0.8542\n",
      "Epoch 22/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2609 - accuracy: 0.9090 - val_loss: 0.5004 - val_accuracy: 0.8346\n",
      "Epoch 23/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2488 - accuracy: 0.9126 - val_loss: 0.5148 - val_accuracy: 0.8289\n",
      "Epoch 24/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2382 - accuracy: 0.9191 - val_loss: 0.4134 - val_accuracy: 0.8631\n",
      "Epoch 25/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2358 - accuracy: 0.9172 - val_loss: 0.4108 - val_accuracy: 0.8680\n",
      "Epoch 26/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2246 - accuracy: 0.9228 - val_loss: 0.4236 - val_accuracy: 0.8612\n",
      "Epoch 27/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2204 - accuracy: 0.9223 - val_loss: 0.4638 - val_accuracy: 0.8512\n",
      "Epoch 28/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2161 - accuracy: 0.9245 - val_loss: 0.4056 - val_accuracy: 0.8641\n",
      "Epoch 29/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2099 - accuracy: 0.9258 - val_loss: 0.4237 - val_accuracy: 0.8629\n",
      "Epoch 30/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.2008 - accuracy: 0.9298 - val_loss: 0.4285 - val_accuracy: 0.8624\n",
      "Epoch 31/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1948 - accuracy: 0.9302 - val_loss: 0.5715 - val_accuracy: 0.8331\n",
      "Epoch 32/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1866 - accuracy: 0.9346 - val_loss: 0.4199 - val_accuracy: 0.8662\n",
      "Epoch 33/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1832 - accuracy: 0.9362 - val_loss: 0.4460 - val_accuracy: 0.8609\n",
      "Epoch 34/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1772 - accuracy: 0.9381 - val_loss: 0.4632 - val_accuracy: 0.8543\n",
      "Epoch 35/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1737 - accuracy: 0.9387 - val_loss: 0.4448 - val_accuracy: 0.8669\n",
      "Epoch 36/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1669 - accuracy: 0.9415 - val_loss: 0.5365 - val_accuracy: 0.8472\n",
      "Epoch 37/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1656 - accuracy: 0.9423 - val_loss: 0.4362 - val_accuracy: 0.8733\n",
      "Epoch 38/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1647 - accuracy: 0.9432 - val_loss: 0.4217 - val_accuracy: 0.8732\n",
      "Epoch 39/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1571 - accuracy: 0.9440 - val_loss: 0.4575 - val_accuracy: 0.8696\n",
      "Epoch 40/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.1544 - accuracy: 0.9456 - val_loss: 0.4733 - val_accuracy: 0.8637\n",
      "Epoch 41/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1527 - accuracy: 0.9464 - val_loss: 0.4313 - val_accuracy: 0.8671\n",
      "Epoch 42/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1472 - accuracy: 0.9478 - val_loss: 0.4325 - val_accuracy: 0.8682\n",
      "Epoch 43/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1448 - accuracy: 0.9490 - val_loss: 0.4116 - val_accuracy: 0.8792\n",
      "Epoch 44/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1430 - accuracy: 0.9493 - val_loss: 0.4165 - val_accuracy: 0.8773\n",
      "Epoch 45/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.1396 - accuracy: 0.9499 - val_loss: 0.4112 - val_accuracy: 0.8793\n",
      "Epoch 46/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.1354 - accuracy: 0.9527 - val_loss: 0.5242 - val_accuracy: 0.8540\n",
      "Epoch 47/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.1332 - accuracy: 0.9526 - val_loss: 0.4335 - val_accuracy: 0.8760\n",
      "Epoch 48/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.1309 - accuracy: 0.9537 - val_loss: 0.3729 - val_accuracy: 0.8917\n",
      "Epoch 49/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.1221 - accuracy: 0.9566 - val_loss: 0.4167 - val_accuracy: 0.8813\n",
      "Epoch 50/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.1276 - accuracy: 0.9553 - val_loss: 0.3837 - val_accuracy: 0.8852\n",
      "Epoch 51/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1221 - accuracy: 0.9576 - val_loss: 0.4161 - val_accuracy: 0.8780\n",
      "Epoch 52/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.1186 - accuracy: 0.9583 - val_loss: 0.4745 - val_accuracy: 0.8704\n",
      "Epoch 53/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1175 - accuracy: 0.9586 - val_loss: 0.4312 - val_accuracy: 0.8804\n",
      "Epoch 54/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.1153 - accuracy: 0.9588 - val_loss: 0.4433 - val_accuracy: 0.8731\n",
      "Epoch 55/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1121 - accuracy: 0.9607 - val_loss: 0.4094 - val_accuracy: 0.8878\n",
      "Epoch 56/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1120 - accuracy: 0.9602 - val_loss: 0.4007 - val_accuracy: 0.8839\n",
      "Epoch 57/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1047 - accuracy: 0.9622 - val_loss: 0.4460 - val_accuracy: 0.8792\n",
      "Epoch 58/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.1109 - accuracy: 0.9611 - val_loss: 0.3956 - val_accuracy: 0.8888\n",
      "Epoch 59/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1033 - accuracy: 0.9643 - val_loss: 0.4376 - val_accuracy: 0.8816\n",
      "Epoch 60/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1044 - accuracy: 0.9625 - val_loss: 0.4276 - val_accuracy: 0.8828\n",
      "Epoch 61/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.1021 - accuracy: 0.9640 - val_loss: 0.3891 - val_accuracy: 0.8908\n",
      "Epoch 62/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0986 - accuracy: 0.9646 - val_loss: 0.3901 - val_accuracy: 0.8959\n",
      "Epoch 63/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0994 - accuracy: 0.9653 - val_loss: 0.4675 - val_accuracy: 0.8755\n",
      "Epoch 64/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0986 - accuracy: 0.9651 - val_loss: 0.4422 - val_accuracy: 0.8780\n",
      "Epoch 65/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0972 - accuracy: 0.9655 - val_loss: 0.4979 - val_accuracy: 0.8722\n",
      "Epoch 66/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0942 - accuracy: 0.9664 - val_loss: 0.4203 - val_accuracy: 0.8882\n",
      "Epoch 67/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0924 - accuracy: 0.9673 - val_loss: 0.4118 - val_accuracy: 0.8869\n",
      "Epoch 68/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0928 - accuracy: 0.9670 - val_loss: 0.5311 - val_accuracy: 0.8654\n",
      "Epoch 69/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0892 - accuracy: 0.9680 - val_loss: 0.5157 - val_accuracy: 0.8695\n",
      "Epoch 70/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0952 - accuracy: 0.9674 - val_loss: 0.4127 - val_accuracy: 0.8854\n",
      "Epoch 71/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0860 - accuracy: 0.9696 - val_loss: 0.4979 - val_accuracy: 0.8710\n",
      "Epoch 72/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0917 - accuracy: 0.9672 - val_loss: 0.4107 - val_accuracy: 0.8828\n",
      "Epoch 73/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0851 - accuracy: 0.9693 - val_loss: 0.4021 - val_accuracy: 0.8940\n",
      "Epoch 74/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0880 - accuracy: 0.9696 - val_loss: 0.3916 - val_accuracy: 0.8953\n",
      "Epoch 75/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0836 - accuracy: 0.9701 - val_loss: 0.4492 - val_accuracy: 0.8868\n",
      "Epoch 76/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0817 - accuracy: 0.9707 - val_loss: 0.4948 - val_accuracy: 0.8786\n",
      "Epoch 77/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0854 - accuracy: 0.9712 - val_loss: 0.4471 - val_accuracy: 0.8825\n",
      "Epoch 78/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0782 - accuracy: 0.9722 - val_loss: 0.4019 - val_accuracy: 0.8918\n",
      "Epoch 79/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0804 - accuracy: 0.9713 - val_loss: 0.4549 - val_accuracy: 0.8829\n",
      "Epoch 80/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0788 - accuracy: 0.9718 - val_loss: 0.4958 - val_accuracy: 0.8773\n",
      "Epoch 81/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0770 - accuracy: 0.9732 - val_loss: 0.4053 - val_accuracy: 0.8931\n",
      "Epoch 82/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0767 - accuracy: 0.9732 - val_loss: 0.4533 - val_accuracy: 0.8840\n",
      "Epoch 83/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0751 - accuracy: 0.9736 - val_loss: 0.4695 - val_accuracy: 0.8832\n",
      "Epoch 84/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0772 - accuracy: 0.9726 - val_loss: 0.4082 - val_accuracy: 0.8918\n",
      "Epoch 85/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0736 - accuracy: 0.9748 - val_loss: 0.4234 - val_accuracy: 0.8924\n",
      "Epoch 86/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0738 - accuracy: 0.9736 - val_loss: 0.4340 - val_accuracy: 0.8956\n",
      "Epoch 87/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 0.4913 - val_accuracy: 0.8816\n",
      "Epoch 88/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0735 - accuracy: 0.9746 - val_loss: 0.4510 - val_accuracy: 0.8881\n",
      "Epoch 89/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0689 - accuracy: 0.9751 - val_loss: 0.4530 - val_accuracy: 0.8876\n",
      "Epoch 90/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0731 - accuracy: 0.9735 - val_loss: 0.4320 - val_accuracy: 0.8880\n",
      "Epoch 91/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0702 - accuracy: 0.9758 - val_loss: 0.4184 - val_accuracy: 0.8953\n",
      "Epoch 92/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0674 - accuracy: 0.9762 - val_loss: 0.4217 - val_accuracy: 0.8943\n",
      "Epoch 93/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0678 - accuracy: 0.9765 - val_loss: 0.4065 - val_accuracy: 0.8978\n",
      "Epoch 94/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0693 - accuracy: 0.9755 - val_loss: 0.4717 - val_accuracy: 0.8835\n",
      "Epoch 95/120\n",
      "782/782 [==============================] - 36s 45ms/step - loss: 0.0694 - accuracy: 0.9757 - val_loss: 0.4406 - val_accuracy: 0.8935\n",
      "Epoch 96/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 0.4963 - val_accuracy: 0.8858\n",
      "Epoch 97/120\n",
      "782/782 [==============================] - 35s 45ms/step - loss: 0.0646 - accuracy: 0.9775 - val_loss: 0.4209 - val_accuracy: 0.8983\n",
      "Epoch 98/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0677 - accuracy: 0.9761 - val_loss: 0.4551 - val_accuracy: 0.8876\n",
      "Epoch 99/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0620 - accuracy: 0.9783 - val_loss: 0.4762 - val_accuracy: 0.8893\n",
      "Epoch 100/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0631 - accuracy: 0.9777 - val_loss: 0.4300 - val_accuracy: 0.8937\n",
      "Epoch 101/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0659 - accuracy: 0.9767 - val_loss: 0.4709 - val_accuracy: 0.8880\n",
      "Epoch 102/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0611 - accuracy: 0.9788 - val_loss: 0.4356 - val_accuracy: 0.8948\n",
      "Epoch 103/120\n",
      "782/782 [==============================] - 36s 47ms/step - loss: 0.0653 - accuracy: 0.9773 - val_loss: 0.4569 - val_accuracy: 0.8932\n",
      "Epoch 104/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0572 - accuracy: 0.9800 - val_loss: 0.4793 - val_accuracy: 0.8894\n",
      "Epoch 105/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0654 - accuracy: 0.9771 - val_loss: 0.3979 - val_accuracy: 0.8965\n",
      "Epoch 106/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0604 - accuracy: 0.9794 - val_loss: 0.4176 - val_accuracy: 0.8992\n",
      "Epoch 107/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0586 - accuracy: 0.9790 - val_loss: 0.4537 - val_accuracy: 0.8936\n",
      "Epoch 108/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0645 - accuracy: 0.9777 - val_loss: 0.4086 - val_accuracy: 0.9004\n",
      "Epoch 109/120\n",
      "782/782 [==============================] - 36s 47ms/step - loss: 0.0560 - accuracy: 0.9807 - val_loss: 0.4491 - val_accuracy: 0.8966\n",
      "Epoch 110/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0610 - accuracy: 0.9790 - val_loss: 0.4183 - val_accuracy: 0.8979\n",
      "Epoch 111/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.0556 - accuracy: 0.9804 - val_loss: 0.4110 - val_accuracy: 0.9026\n",
      "Epoch 112/120\n",
      "782/782 [==============================] - 36s 47ms/step - loss: 0.0584 - accuracy: 0.9787 - val_loss: 0.4689 - val_accuracy: 0.8896\n",
      "Epoch 113/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.0579 - accuracy: 0.9799 - val_loss: 0.4341 - val_accuracy: 0.8992\n",
      "Epoch 114/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.0557 - accuracy: 0.9804 - val_loss: 0.4839 - val_accuracy: 0.8903\n",
      "Epoch 115/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.0573 - accuracy: 0.9803 - val_loss: 0.4303 - val_accuracy: 0.8957\n",
      "Epoch 116/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.0550 - accuracy: 0.9813 - val_loss: 0.4575 - val_accuracy: 0.8900\n",
      "Epoch 117/120\n",
      "782/782 [==============================] - 37s 47ms/step - loss: 0.0542 - accuracy: 0.9808 - val_loss: 0.4734 - val_accuracy: 0.8918\n",
      "Epoch 118/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0546 - accuracy: 0.9808 - val_loss: 0.5133 - val_accuracy: 0.8825\n",
      "Epoch 119/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0535 - accuracy: 0.9814 - val_loss: 0.4436 - val_accuracy: 0.8974\n",
      "Epoch 120/120\n",
      "782/782 [==============================] - 36s 46ms/step - loss: 0.0535 - accuracy: 0.9813 - val_loss: 0.4625 - val_accuracy: 0.8930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa3e2b89a20>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,batch_size=batch_size,epochs=120,verbose=1,validation_data=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QP2-tgSW8eRK"
   },
   "source": [
    "Observation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fL2AtI2R8-Zu"
   },
   "source": [
    "From above the model is trained for 120 epoch the highest test accuracy i got is around 0.9026 at 111th epoch this is accuracy which matches what an expected accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABp1cAtN9lKO"
   },
   "source": [
    "when i train for more epoch the test accuaracy will be constant more than 90+ test accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "dhilipvasanth@gmail.com(CNN on CFIR Assingment).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
